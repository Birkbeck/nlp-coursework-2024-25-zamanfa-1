{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09371423-788e-4d8e-b4f9-26aef3f003be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "                             \n",
    "        \n",
    "                             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a830ff5a-c9c2-4950-bd9a-e57595ca1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_novels(path=Path.cwd() / \"p1-texts\" / \"novels\"):\n",
    "    data = []\n",
    "    for file in path.glob(\"*.txt\"):\n",
    "        title_name = file.stem\n",
    "        parts = title_name.split(\"-\")\n",
    "\n",
    "        if len(parts) >= 3:\n",
    "            title = ' '.join(part.replace('_', ' ') for part in parts[:-2])\n",
    "            author = parts[-2].replace('_', ' ')\n",
    "            year = parts[-1]\n",
    "\n",
    "            with file.open(encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            data.append({\n",
    "                'text': text,\n",
    "                'title': title,\n",
    "                'author': author,\n",
    "                'year': int(year)  \n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = df.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ee6b59-001d-4edb-8ac7-ebb303037003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_ttr(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    if not words:\n",
    "        return 0\n",
    "\n",
    "    types = set(words)\n",
    "    ttr = len(types) / len(words)\n",
    "    return ttr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0cfe9b-c735-4174-90d0-46895a1b2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef7833c-7bf1-4dc6-9f7d-c46ec91e976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syl(word, d):\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        return len([pron for pron in d[word][0] if pron[-1].isdigit()])\n",
    "    \n",
    "    vowels = 'aeiouy'\n",
    "    syllable_count = 0\n",
    "    prev_letter_was_vowel = False\n",
    "\n",
    "    for letter in word:\n",
    "        if letter in vowels and not prev_letter_was_vowel:\n",
    "            syllable_count += 1\n",
    "            prev_letter_was_vowel = True\n",
    "        else:\n",
    "            prev_letter_was_vowel = False\n",
    "\n",
    "    return max(1, syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa607a02-eb7a-4ae2-8749-0d5b9e322b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_level(text, d):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend([word.lower() for word in word_tokenize(sentence) if word not in string.punctuation and word.isalpha()])\n",
    "        \n",
    "    if not words or not sentences:\n",
    "        return 0\n",
    "\n",
    "    total_syllables = sum(count_syl(word, d) for word in words)\n",
    "    total_words = len(words)\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    fk_score = (0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59)\n",
    "\n",
    "    return fk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17451f63-62d2-4193-904a-b4376fd0e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    cmudict = nltk.corpus.cmudict.dict()\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], cmudict), 4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b7ee24c-0331-4e78-8fe4-0397a92907e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(df, store_path=Path.cwd() / \"pickle\", out_name=\"parsed.pickle\"):\n",
    "    store_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = store_path / out_name\n",
    "\n",
    "    df['doc'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print(f\"Saved parsed DataFrame to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b463697-8b02-4843-b257-b8c7cc5dea51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'novels_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnovels_df\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'novels_df' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    uncomment the following lines to run the functions once you have completed them\n",
    "    \"\"\"\n",
    "    path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    print(path)\n",
    "    df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    print(df.head())\n",
    "    nltk.download(\"cmudict\")\n",
    "    nltk.download(\"punkt\")\n",
    "    #parse(df)\n",
    "    #print(df.head())\n",
    "    print(get_ttrs(df))\n",
    "    print(get_fks(df))\n",
    "    df = pd.read_pickle(Path.cwd() / \"pickles\" /\"parsed.pickle\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(adjective_counts(row[\"doc\"]))\n",
    "        print(\"\\n\")       \n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_count(row[\"doc\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"doc\"], \"hear\"))\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
