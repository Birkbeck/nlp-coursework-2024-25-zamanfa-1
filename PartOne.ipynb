{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09371423-788e-4d8e-b4f9-26aef3f003be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "                             \n",
    "        \n",
    "                             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a830ff5a-c9c2-4950-bd9a-e57595ca1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_novels(path=Path.cwd() / \"p1-texts\" / \"novels\"):\n",
    "    data = []\n",
    "    for file in path.glob(\"*.txt\"):\n",
    "        title_name = file.stem\n",
    "        parts = title_name.split(\"-\")\n",
    "\n",
    "        if len(parts) >= 3:\n",
    "            title = ' '.join(part.replace('_', ' ') for part in parts[:-2])\n",
    "            author = parts[-2].replace('_', ' ')\n",
    "            year = parts[-1]\n",
    "\n",
    "            with file.open(encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            data.append({\n",
    "                'text': text,\n",
    "                'title': title,\n",
    "                'author': author,\n",
    "                'year': int(year)  \n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = df.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05ee6b59-001d-4edb-8ac7-ebb303037003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_ttr(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    if not words:\n",
    "        return 0\n",
    "\n",
    "    types = set(words)\n",
    "    ttr = len(types) / len(words)\n",
    "    return ttr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa0cfe9b-c735-4174-90d0-46895a1b2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ef7833c-7bf1-4dc6-9f7d-c46ec91e976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syl(word, d):\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        return len([pron for pron in d[word][0] if pron[-1].isdigit()])\n",
    "    \n",
    "    vowels = 'aeiouy'\n",
    "    syllable_count = 0\n",
    "    prev_letter_was_vowel = False\n",
    "\n",
    "    for letter in word:\n",
    "        if letter in vowels and not prev_letter_was_vowel:\n",
    "            syllable_count += 1\n",
    "            prev_letter_was_vowel = True\n",
    "        else:\n",
    "            prev_letter_was_vowel = False\n",
    "\n",
    "    return max(1, syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa607a02-eb7a-4ae2-8749-0d5b9e322b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_level(text, d):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend([word.lower() for word in word_tokenize(sentence) if word not in string.punctuation and word.isalpha()])\n",
    "        \n",
    "    if not words or not sentences:\n",
    "        return 0\n",
    "\n",
    "    total_syllables = sum(count_syl(word, d) for word in words)\n",
    "    total_words = len(words)\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    fk_score = (0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59)\n",
    "\n",
    "    return fk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17451f63-62d2-4193-904a-b4376fd0e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    cmudict = nltk.corpus.cmudict.dict()\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], cmudict), 4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b7ee24c-0331-4e78-8fe4-0397a92907e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(df, store_path=Path.cwd() / \"pickle\", out_name=\"parsed.pickle\"):\n",
    "    store_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = store_path / out_name\n",
    "\n",
    "    df['doc'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print(f\"Saved parsed DataFrame to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b4a012d-917f-4236-9d94-0ad8cd54d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parsed.pickle']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(path.cwd() / \"pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4751f5b-8322-4f43-9bb9-eacb8af6bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'title', 'author', 'year', 'doc'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(Path.cwd() / \"pickle\" / \"parsed.pickle\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c0b8d4b-167d-4670-9977-e505248c6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjective_counts(doc):\n",
    "    adjectives = [token.text.lower() for token in doc if token.pos_ == \"ADJ\"]\n",
    "\n",
    "    return Counter(adjectives).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "369ac3b5-655b-4d8c-9181-2143833a658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects_by_verb_count(doc, verb):\n",
    "    subject_counter = Counter()\n",
    "\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() == verb and token.pos_ == \"VERB\":\n",
    "            subject_counter.update(\n",
    "                child.lemma_lower()\n",
    "                for child in token.children\n",
    "                if child.dep_ == \"nsubj\"\n",
    "            )\n",
    "\n",
    "    return subject_counter.most_common(10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6b6f6c3-dbb2-4572-a444-83a9f12e05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    verb = target_verb.lower()\n",
    "\n",
    "    subj_verb_counts = Counter()\n",
    "    subj_counts = Counter()\n",
    "    verb_counts = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\" and token.lemma_.lower() == verb:\n",
    "            verb_counts += 1\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"nsubj\":\n",
    "                    subj = child.lemma_.lower()\n",
    "                    subj_verb_counts[subj] += 1\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"NOUN\", \"PROPN\", \"PRON\"):\n",
    "            subj_counts[token.lemma_.lower()] += 1\n",
    "\n",
    "    total_tokens = len(doc)\n",
    "\n",
    "    pmi_scores = {}\n",
    "    for subj in subj_verb_counts:\n",
    "        p_subj_verb = subj_verb_counts[subj] / total_tokens\n",
    "        p_subj = subj_counts[subj] / total_tokens\n",
    "        p_verb = verb_counts / total_tokens\n",
    "        if p_subj > 0 and p_verb > 0 and p_subj_verb > 0:\n",
    "            pmi = math.log2(p_subj_verb / (p_subj * p_verb))\n",
    "            pmi_scores[subj] = pmi\n",
    "\n",
    "    return sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b463697-8b02-4843-b257-b8c7cc5dea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaman\\OneDrive\\nlp-coursework-2024-25-zamanfa-1-main\\nlp-coursework-2024-25-zamanfa-1-1\\p1-texts\\novels\n",
      "                                                text                  title  \\\n",
      "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...  Sense and Sensibility   \n",
      "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North and South   \n",
      "2  Book the First--Recalled to Life\\n\\n\\n\\n\\nI. T...   A Tale of Two Cities   \n",
      "3  SAMUEL BUTLER.\\nAugust 7, 1901\\n\\nCHAPTER I: W...                Erewhon   \n",
      "4  THE AMERICAN\\n\\nby Henry James\\n\\n\\n1877\\n\\n\\n...           The American   \n",
      "\n",
      "    author  year  \n",
      "0   Austen  1811  \n",
      "1  Gaskell  1855  \n",
      "2  Dickens  1858  \n",
      "3   Butler  1872  \n",
      "4    James  1877  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\zaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sense and Sensibility': 0.052847302442989776, 'North and South': 0.0549040694681204, 'A Tale of Two Cities': 0.07072694469399422, 'Erewhon': 0.09151270564132943, 'The American': 0.06381607058523676, 'Dorian Gray': 0.08355234620193412, 'Tess of the DUrbervilles': 0.07778957979554696, 'The Golden Bowl': 0.047475476259872806, 'The Secret Garden': 0.05847231570812455, 'Portrait of the Artist': 0.10472745625841184, 'The Black Moth': 0.07866588875923765, 'Orlando': 0.1137245917497168, 'Blood Meridian': 0.08568897067593587}\n",
      "{'Sense and Sensibility': 10.898, 'North and South': 6.6594, 'A Tale of Two Cities': 9.8466, 'Erewhon': 14.6827, 'The American': 7.9951, 'Dorian Gray': 4.9526, 'Tess of the DUrbervilles': 7.6353, 'The Golden Bowl': 12.4474, 'The Secret Garden': 4.6623, 'Portrait of the Artist': 6.454, 'The Black Moth': 4.2235, 'Orlando': 9.5473, 'Blood Meridian': 5.6416}\n",
      "Sense and Sensibility\n",
      "[('own', 267), ('such', 202), ('more', 196), ('other', 185), ('good', 153), ('great', 148), ('little', 146), ('sure', 129), ('much', 109), ('young', 103)]\n",
      "\n",
      "\n",
      "North and South\n",
      "[('little', 333), ('good', 286), ('own', 247), ('more', 224), ('other', 196), ('sure', 193), ('poor', 192), ('great', 178), ('last', 176), ('old', 173)]\n",
      "\n",
      "\n",
      "A Tale of Two Cities\n",
      "[('little', 253), ('good', 195), ('other', 181), ('old', 166), ('great', 161), ('own', 159), ('more', 151), ('many', 140), ('such', 123), ('last', 117)]\n",
      "\n",
      "\n",
      "Erewhon\n",
      "[('own', 148), ('other', 136), ('great', 112), ('many', 105), ('good', 103), ('more', 101), ('little', 94), ('few', 89), ('such', 81), ('much', 78)]\n",
      "\n",
      "\n",
      "The American\n",
      "[('little', 293), ('good', 268), ('old', 261), ('great', 251), ('young', 178), ('other', 139), ('last', 132), ('own', 127), ('more', 118), ('better', 99)]\n",
      "\n",
      "\n",
      "Dorian Gray\n",
      "[('own', 130), ('good', 120), ('little', 81), ('great', 75), ('young', 72), ('other', 72), ('more', 67), ('dear', 63), ('afraid', 61), ('old', 58)]\n",
      "\n",
      "\n",
      "Tess of the DUrbervilles\n",
      "[('other', 224), ('little', 203), ('own', 196), ('more', 193), ('old', 162), ('such', 161), ('good', 144), ('few', 113), ('last', 111), ('same', 108)]\n",
      "\n",
      "\n",
      "The Golden Bowl\n",
      "[('little', 456), ('own', 364), ('more', 349), ('other', 349), ('such', 216), ('good', 197), ('least', 197), ('great', 192), ('much', 191), ('same', 190)]\n",
      "\n",
      "\n",
      "The Secret Garden\n",
      "[('little', 221), ('other', 110), ('good', 94), ('old', 76), ('long', 74), ('big', 71), ('great', 70), ('young', 69), ('sure', 69), ('own', 58)]\n",
      "\n",
      "\n",
      "Portrait of the Artist\n",
      "[('little', 122), ('old', 109), ('own', 86), ('good', 85), ('first', 84), ('dark', 83), ('long', 71), ('last', 66), ('other', 65), ('cold', 57)]\n",
      "\n",
      "\n",
      "The Black Moth\n",
      "[('little', 178), ('good', 101), ('dear', 87), ('other', 87), ('more', 85), ('great', 83), ('own', 67), ('sure', 59), ('last', 58), ('old', 57)]\n",
      "\n",
      "\n",
      "Orlando\n",
      "[('great', 140), ('other', 118), ('little', 108), ('such', 106), ('old', 86), ('own', 81), ('many', 75), ('more', 74), ('same', 72), ('whole', 55)]\n",
      "\n",
      "\n",
      "Blood Meridian\n",
      "[('old', 257), ('other', 158), ('small', 157), ('dead', 131), ('little', 130), ('dark', 121), ('naked', 91), ('great', 89), ('black', 86), ('more', 78)]\n",
      "\n",
      "\n",
      "Sense and Sensibility\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'lemma_lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msubjects_by_verb_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m, in \u001b[0;36msubjects_by_verb_count\u001b[1;34m(doc, verb)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m verb \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m         \u001b[43msubject_counter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_lower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdep_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnsubj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subject_counter\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\collections\\__init__.py:699\u001b[0m, in \u001b[0;36mCounter.update\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 699\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "Cell \u001b[1;32mIn[55], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m verb \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mpos_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      6\u001b[0m         subject_counter\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m----> 7\u001b[0m             \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma_lower\u001b[49m()\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mchildren\n\u001b[0;32m      9\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m child\u001b[38;5;241m.\u001b[39mdep_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnsubj\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m         )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subject_counter\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'lemma_lower'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    uncomment the following lines to run the functions once you have completed them\n",
    "    \"\"\"\n",
    "    path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    print(path)\n",
    "    df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    print(df.head())\n",
    "    nltk.download(\"cmudict\")\n",
    "    nltk.download(\"punkt\")\n",
    "    #parse(df)\n",
    "    #print(df.head())\n",
    "    print(get_ttrs(df))\n",
    "    print(get_fks(df))\n",
    "    df = pd.read_pickle(Path.cwd() / \"pickle\" /\"parsed.pickle\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(adjective_counts(row[\"doc\"]))\n",
    "        print(\"\\n\")       \n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_count(row[\"doc\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"doc\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
