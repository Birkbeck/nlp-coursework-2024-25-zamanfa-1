1) d)
One limitation Flesch-Kincaid (FK) score has is that it relies on syllable counts optimised for standard American English pronunciation. It becomes unreliable for :
- Text with non-native English structures
- Dialects with different phonetic rules
The syllable-counting heuristic often misjudges such cases which may skew the estimates.

Another limitation is that Fk measures syntactic complexity (word/sentence length) but ignores semantic difficulty:
- Technical papers with concise sentences but complex concepts may score low
- Poetry or lyrics with intentional syntactic complexity may score high despite being conceptually simple
The formula cannot account for domain-specific terminology or abract ideas that affect real comprehension diffculty.

FK score works best for standardized English with predictable vocabulary and linear syntax, not for non-conventional texts.

2) f)
The custom tokenizer function was implemented using spaCy (en_core_web_sm) and designed to extract features by focusing on the most meaningful tokens in each speech.
In my function, I used:
- Lemmatization which ensures each word is reduced to its base form (e.g "hearing" to "hear") to reduce feature redundancy.
- Lowercasing to ensure case-insensitive matching.
- Stopword removal which removes common words like "the", "and" and so on which does not add significant value.
- Punctuation and digit filtering to remove non-informative characters like "1234", ",", ".", etc.
- Length filtering which discards short words (in my case 2 or fewer characters) to reduce noise.

This custom tokenizer leads to a cleaner, more informative feature space for classification, especially when it is combined with TF-IDF vectorization and bigram inclusion.

The macro F1 scores from the different models show a clear performance improvement when using the custom tokenizer compared to the default tokenizer and n-gram approach. Overall, all models improved in balance and consistency with the lowest macro F1 score being 0.405 from Naive Bayes, likely due to its assumption of feature independence, which does not hold well with bigrams or complex token interactions. This is still significantly greater then the macro F1 score calculated in part d for SVM with a score of 0.219. The MLP Classifier achieved the best performance with a macro F1 score of 0.672, showing strong performance across major and minor parties. This demonstrates that preprocessing data, especially with domain-aware tokenization, can signifiantly improve classification quality even with a limited feature set of 3000 features.
