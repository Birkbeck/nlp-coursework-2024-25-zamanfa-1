1) d)
One limitation Flesch-Kincaid (FK) score has is that it relies on syllable counts optimised for standard American English pronunciation. It becomes unreliable for:
- Text with non-native English structures
- Dialects with different phonetic rules
The syllable-counting heuristic often misjudges such cases which may skew the estimates.

Another limitation is that Fk measures syntactic complexity (word/sentence length) but ignores semantic difficulty:
- Technical papers with concise sentences but complex concepts may score low
- Poetry or lyrics with intentional syntactic complexity may score high despite being conceptually simple
The formula cannot account for domain-specific terminology or abract ideas that affect real comprehension diffculty.

FK score works best for standardized English with predictable vocabulary and linear syntax, not for non-conventional texts.

2) f)
The custom tokenizer function was implemented using spaCy (en_core_web_sm) and designed to extract features by focusing on the most meaningful tokens in each speech.
In my function, I used:
- Lemmatization which ensures each word is reduced to its base form (e.g "hearing" to "hear") to reduce feature redundancy.
- Lowercasing to ensure case-insensitive matching.
- Stopword removal which removes common words like "the", "and" and so on which does not add significant value.
- Punctuation and digit filtering to remove non-informative characters like "1234", ",", ".", etc.
- Length filtering which discards short words (in my case 2 or fewer characters) to reduce noise.

The custom tokenizer was used with a TfidfVectorizer (using unigrams and bigrams, max_features=3000) to train the same two classifiers used in earlier parts. The macro F1 score I managed to achieve was 0.4798 for RandomForest and 0.5751 for SVM. 

RandomForest achieved its best result using my custom tokenizer, showing that the tokenizer helped clean up noise and improve signal.
SVM scored slightly lower than in part (c) but the custom tokenizer still achieved a competitive performance, much better than the drop in part (d) due to overfitting with the trigrams.

In conclusion, the custom tokenizer proved effective in balancing classification performance with linguistic efficiency. It improved the model robustness by reducing noise and dimensionality and it preserved key patterns by focusing on lemmas and informative tokens.
